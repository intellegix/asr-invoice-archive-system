"""
ASR Records - QB Organization Validation & Claude Vision Verification System
Comprehensive validation and enhancement using Claude Vision API
"""

import os
import sys
import json
import time
import logging
import argparse
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import asyncio
import hashlib
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# Add parent directory for imports
sys.path.insert(0, str(Path(__file__).parent))

from config.settings import settings

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('qb_validation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ValidationResult:
    """Result of QB organization validation."""
    target_directory: str
    scan_timestamp: str
    total_files: int
    business_documents: int
    system_files: int
    compliance_score: float
    path_violations: int
    issues_found: List[Dict[str, Any]]
    recommendations: List[str]
    cost_estimate_usd: float
    processing_time_seconds: float

@dataclass
class RemediationPlan:
    """Plan for remediation actions."""
    cleanup_actions: List[Dict[str, Any]]
    path_fixes: List[Dict[str, Any]]
    folder_creations: List[str]
    file_moves: List[Dict[str, Any]]
    estimated_time_minutes: int
    safety_backup_required: bool

class QBChartOfAccounts:
    """QuickBooks Chart of Accounts structure definition."""

    @staticmethod
    def get_qb_structure() -> Dict[str, List[str]]:
        """Get complete QB Chart of Accounts folder structure."""
        return {
            "1000-ASSETS": [
                "1020-Banking/Corporate_Checking",
                "1020-Banking/Savings_Accounts",
                "1020-Banking/Investment_Accounts",
                "1200-Accounts_Receivable",
                "1400-Current_Assets",
                "1440-Fixed_Assets/Tools_Equipment",
                "1440-Fixed_Assets/Buildings",
                "1440-Fixed_Assets/Vehicles",
                "1440-Fixed_Assets/Office_Equipment",
                "1700-1900-LongTerm_Assets"
            ],
            "2000-LIABILITIES": [
                "2000-Accounts_Payable",
                "2100-2200-Current_Liabilities",
                "2400-2430-Loans_Payable",
                "2139-2175-Credit_Cards/AmEx",
                "2139-2175-Credit_Cards/Capital_One",
                "2139-2175-Credit_Cards/Home_Depot",
                "2139-2175-Credit_Cards/Wells_Fargo",
                "2536-2602-Equipment_Leases"
            ],
            "3000-EQUITY": [],
            "4000-INCOME": [
                "4060-Construction_Sales",
                "4100-Sales",
                "4110-Repairs",
                "4120-Reroofing"
            ],
            "5000-COGS": [
                "5010-Roofing_Materials",
                "5020-Direct_Labor",
                "5030-Payroll_Taxes"
            ],
            "6000-OPERATING_EXPENSES": [
                "6075-6190-Insurance_Related",
                "6200-6290-Professional_Services",
                "6300-6400-Utilities_Rent",
                "6500-6600-Office_Admin",
                "6700-6750-Equipment_Subcontractors",
                "6800-6900-Taxes_Other_Expenses"
            ]
        }

    @staticmethod
    def get_vendor_mappings() -> Dict[str, str]:
        """Get vendor to GL account mappings."""
        return {
            # Insurance Providers
            "liberty_mutual": "6075-6190-Insurance_Related",
            "state_farm": "6075-6190-Insurance_Related",
            "allstate": "6075-6190-Insurance_Related",
            "farmers": "6075-6190-Insurance_Related",

            # Materials/Supplies
            "home_depot": "5010-Roofing_Materials",
            "lowes": "5010-Roofing_Materials",
            "abc_supply": "5010-Roofing_Materials",
            "beacon": "5010-Roofing_Materials",

            # Utilities
            "sdge": "6300-6400-Utilities_Rent",
            "cox": "6300-6400-Utilities_Rent",
            "verizon": "6300-6400-Utilities_Rent",
            "att": "6300-6400-Utilities_Rent",

            # Professional Services
            "quickbooks": "6200-6290-Professional_Services",
            "intuit": "6200-6290-Professional_Services",
            "adobe": "6200-6290-Professional_Services",
            "microsoft": "6200-6290-Professional_Services",

            # Banking/Financial
            "wells_fargo": "2139-2175-Credit_Cards/Wells_Fargo",
            "capital_one": "2139-2175-Credit_Cards/Capital_One",
            "american_express": "2139-2175-Credit_Cards/AmEx",
            "amex": "2139-2175-Credit_Cards/AmEx"
        }

class QBValidationEngine:
    """Main validation engine for QB organization compliance."""

    def __init__(self, target_directory: str, dry_run: bool = True):
        """Initialize validation engine."""
        self.target_directory = Path(target_directory)
        self.dry_run = dry_run
        self.qb_structure = QBChartOfAccounts.get_qb_structure()
        self.vendor_mappings = QBChartOfAccounts.get_vendor_mappings()
        self.start_time = time.time()

        logger.info(f"QB Validation Engine initialized")
        logger.info(f"Target: {self.target_directory}")
        logger.info(f"Mode: {'DRY RUN' if dry_run else 'LIVE EXECUTION'}")

    async def run_comprehensive_validation(self) -> ValidationResult:
        """Execute comprehensive QB validation process."""
        logger.info("[START] Starting comprehensive QB validation...")

        # Phase 1: Directory Inventory
        logger.info("[SCAN] Phase 1: Directory Inventory")
        inventory = await self._scan_directory_inventory()

        # Phase 2: QB Structure Compliance Check
        logger.info("üèóÔ∏è  Phase 2: QB Structure Compliance")
        structure_compliance = await self._check_qb_structure_compliance()

        # Phase 3: File Organization Analysis
        logger.info("üìÇ Phase 3: File Organization Analysis")
        organization_analysis = await self._analyze_file_organization(inventory)

        # Phase 4: Path Length Validation
        logger.info("üìè Phase 4: Path Length Validation")
        path_validation = await self._validate_path_lengths(inventory)

        # Phase 5: System File Cleanup Analysis
        logger.info("üßπ Phase 5: System File Cleanup")
        cleanup_analysis = await self._analyze_system_files(inventory)

        # Phase 6: Sample Re-Classification (if enabled)
        logger.info("üîç Phase 6: Classification Accuracy")
        classification_validation = await self._validate_classification_accuracy(inventory)

        # Compile results
        processing_time = time.time() - self.start_time

        # Calculate compliance score
        compliance_score = self._calculate_compliance_score(
            structure_compliance, organization_analysis,
            path_validation, cleanup_analysis, classification_validation
        )

        # Generate recommendations
        recommendations = self._generate_recommendations(
            structure_compliance, organization_analysis,
            path_validation, cleanup_analysis, classification_validation
        )

        # Compile issues
        all_issues = []
        all_issues.extend(structure_compliance.get("issues", []))
        all_issues.extend(organization_analysis.get("issues", []))
        all_issues.extend(path_validation.get("issues", []))
        all_issues.extend(cleanup_analysis.get("issues", []))
        all_issues.extend(classification_validation.get("issues", []))

        result = ValidationResult(
            target_directory=str(self.target_directory),
            scan_timestamp=datetime.now().isoformat(),
            total_files=inventory["total_files"],
            business_documents=inventory["business_documents"],
            system_files=inventory["system_files"],
            compliance_score=compliance_score,
            path_violations=path_validation.get("violations", 0),
            issues_found=all_issues,
            recommendations=recommendations,
            cost_estimate_usd=classification_validation.get("estimated_cost", 0.0),
            processing_time_seconds=processing_time
        )

        # Save validation report
        await self._save_validation_report(result)

        logger.info(f"‚úÖ Validation complete! Score: {compliance_score:.1f}%")
        return result

    async def _scan_directory_inventory(self) -> Dict[str, Any]:
        """Scan target directory and create comprehensive inventory."""
        logger.info("[SCAN] Scanning directory inventory...")

        if not self.target_directory.exists():
            logger.error(f"‚ùå Target directory not found: {self.target_directory}")
            return {"error": "Directory not found", "total_files": 0}

        inventory = {
            "total_files": 0,
            "business_documents": 0,
            "system_files": 0,
            "temp_files": 0,
            "directories": 0,
            "file_types": {},
            "size_distribution": {},
            "files_by_type": {
                "business": [],
                "system": [],
                "temp": []
            }
        }

        # Scan all files recursively
        for root, dirs, files in os.walk(self.target_directory):
            inventory["directories"] += len(dirs)

            for file in files:
                file_path = Path(root) / file
                inventory["total_files"] += 1

                # Classify file type
                if self._is_temp_file(file):
                    inventory["temp_files"] += 1
                    inventory["files_by_type"]["temp"].append(str(file_path))
                elif self._is_system_file(file):
                    inventory["system_files"] += 1
                    inventory["files_by_type"]["system"].append(str(file_path))
                elif self._is_business_document(file_path):
                    inventory["business_documents"] += 1
                    inventory["files_by_type"]["business"].append(str(file_path))

                # Track file extensions
                ext = file_path.suffix.lower()
                inventory["file_types"][ext] = inventory["file_types"].get(ext, 0) + 1

                # Track file sizes
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    size_category = self._get_size_category(size_mb)
                    inventory["size_distribution"][size_category] = \
                        inventory["size_distribution"].get(size_category, 0) + 1
                except:
                    pass

        logger.info(f"[SCAN] Inventory complete: {inventory['total_files']} files, "
                   f"{inventory['business_documents']} business docs, "
                   f"{inventory['system_files']} system files, "
                   f"{inventory['temp_files']} temp files")

        return inventory

    async def _check_qb_structure_compliance(self) -> Dict[str, Any]:
        """Check compliance with QB Chart of Accounts structure."""
        logger.info("üèóÔ∏è  Checking QB structure compliance...")

        compliance = {
            "total_accounts": len(self._get_all_qb_folders()),
            "existing_folders": 0,
            "missing_folders": [],
            "unexpected_folders": [],
            "issues": [],
            "compliance_percentage": 0.0
        }

        # Check for expected QB folders
        expected_folders = self._get_all_qb_folders()

        for folder_path in expected_folders:
            full_path = self.target_directory / folder_path
            if full_path.exists():
                compliance["existing_folders"] += 1
            else:
                compliance["missing_folders"].append(folder_path)

        # Check for unexpected folders in root
        if self.target_directory.exists():
            for item in self.target_directory.iterdir():
                if item.is_dir():
                    folder_name = item.name
                    # Check if this is part of expected QB structure
                    if not any(folder_name in expected for expected in expected_folders):
                        # Check if it's one of the basic folders (OPEN/CLOSED)
                        if folder_name not in ["OPEN Billing", "CLOSED Billing",
                                             "Open Payable", "Closed Payable"]:
                            compliance["unexpected_folders"].append(folder_name)

        # Calculate compliance percentage
        if compliance["total_accounts"] > 0:
            compliance["compliance_percentage"] = \
                (compliance["existing_folders"] / compliance["total_accounts"]) * 100

        # Generate issues
        if compliance["missing_folders"]:
            compliance["issues"].append({
                "type": "missing_qb_folders",
                "severity": "medium",
                "description": f"{len(compliance['missing_folders'])} QB folders missing",
                "details": compliance["missing_folders"][:10]  # Limit for readability
            })

        if compliance["unexpected_folders"]:
            compliance["issues"].append({
                "type": "unexpected_folders",
                "severity": "low",
                "description": f"{len(compliance['unexpected_folders'])} unexpected folders",
                "details": compliance["unexpected_folders"]
            })

        logger.info(f"üèóÔ∏è  QB structure compliance: {compliance['compliance_percentage']:.1f}%")
        return compliance

    async def _analyze_file_organization(self, inventory: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze file organization quality."""
        logger.info("üìÇ Analyzing file organization...")

        analysis = {
            "properly_organized": 0,
            "misplaced_files": 0,
            "naming_violations": 0,
            "issues": [],
            "organization_score": 0.0
        }

        # Sample a subset of business documents for analysis
        business_docs = inventory["files_by_type"]["business"]
        sample_size = min(100, len(business_docs))  # Limit sample for cost control

        if sample_size > 0:
            sample_docs = business_docs[:sample_size]

            for doc_path in sample_docs:
                doc = Path(doc_path)

                # Check naming convention
                if not self._follows_qb_naming_convention(doc.name):
                    analysis["naming_violations"] += 1

                # Check if file is in appropriate folder
                if not self._is_in_appropriate_qb_folder(doc):
                    analysis["misplaced_files"] += 1
                else:
                    analysis["properly_organized"] += 1

        # Calculate organization score
        total_analyzed = sample_size
        if total_analyzed > 0:
            properly_organized_pct = (analysis["properly_organized"] / total_analyzed) * 100
            naming_compliance_pct = ((total_analyzed - analysis["naming_violations"]) / total_analyzed) * 100
            analysis["organization_score"] = (properly_organized_pct + naming_compliance_pct) / 2

        # Generate issues
        if analysis["naming_violations"] > 0:
            analysis["issues"].append({
                "type": "naming_violations",
                "severity": "medium",
                "description": f"{analysis['naming_violations']} files don't follow QB naming convention",
                "sample_size": sample_size
            })

        if analysis["misplaced_files"] > 0:
            analysis["issues"].append({
                "type": "misplaced_files",
                "severity": "high",
                "description": f"{analysis['misplaced_files']} files in wrong QB folders",
                "sample_size": sample_size
            })

        logger.info(f"üìÇ Organization analysis: {analysis['organization_score']:.1f}% score")
        return analysis

    async def _validate_path_lengths(self, inventory: Dict[str, Any]) -> Dict[str, Any]:
        """Validate Windows path length compliance."""
        logger.info("üìè Validating path lengths...")

        validation = {
            "violations": 0,
            "longest_path": 0,
            "violation_details": [],
            "issues": []
        }

        # Check all files for path length violations
        all_files = []
        all_files.extend(inventory["files_by_type"]["business"])
        all_files.extend(inventory["files_by_type"]["system"])
        all_files.extend(inventory["files_by_type"]["temp"])

        for file_path in all_files:
            path_length = len(str(file_path))
            validation["longest_path"] = max(validation["longest_path"], path_length)

            # Windows has 260 character limit, we use 250 for safety
            if path_length > 250:
                validation["violations"] += 1
                validation["violation_details"].append({
                    "file": file_path,
                    "length": path_length,
                    "excess": path_length - 250
                })

        # Generate issues
        if validation["violations"] > 0:
            validation["issues"].append({
                "type": "path_length_violations",
                "severity": "high",
                "description": f"{validation['violations']} files exceed Windows path limits",
                "longest_path": validation["longest_path"]
            })

        logger.info(f"üìè Path validation: {validation['violations']} violations found")
        return validation

    async def _analyze_system_files(self, inventory: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze system files requiring cleanup."""
        logger.info("üßπ Analyzing system file cleanup needs...")

        analysis = {
            "temp_files_found": inventory["temp_files"],
            "system_files_found": inventory["system_files"],
            "cleanup_actions": [],
            "issues": []
        }

        # Analyze temp files
        temp_files = inventory["files_by_type"]["temp"]
        for temp_file in temp_files:
            analysis["cleanup_actions"].append({
                "action": "delete",
                "file": temp_file,
                "reason": "temporary file"
            })

        # Analyze system files
        system_files = inventory["files_by_type"]["system"]
        for system_file in system_files:
            analysis["cleanup_actions"].append({
                "action": "delete",
                "file": system_file,
                "reason": "system file"
            })

        # Generate issues
        total_cleanup = len(analysis["cleanup_actions"])
        if total_cleanup > 0:
            analysis["issues"].append({
                "type": "system_file_cleanup",
                "severity": "medium",
                "description": f"{total_cleanup} system/temp files require cleanup",
                "breakdown": f"{inventory['temp_files']} temp, {inventory['system_files']} system"
            })

        logger.info(f"üßπ System file analysis: {total_cleanup} files need cleanup")
        return analysis

    async def _validate_classification_accuracy(self, inventory: Dict[str, Any]) -> Dict[str, Any]:
        """Validate document classification accuracy with sample re-classification."""
        logger.info("üîç Validating classification accuracy...")

        validation = {
            "sample_size": 0,
            "reclassifications": 0,
            "accuracy_score": 100.0,
            "estimated_cost": 0.0,
            "issues": []
        }

        # For dry run, just estimate cost and skip actual re-classification
        business_docs = inventory["files_by_type"]["business"]
        sample_size = min(250, len(business_docs))  # 250 document sample

        if sample_size > 0:
            validation["sample_size"] = sample_size

            # Cost estimation for Haiku model
            # Approximately $0.0005 per document (from plan)
            validation["estimated_cost"] = sample_size * 0.0005

            # For dry run, simulate some reclassification needs
            if self.dry_run:
                # Estimate 5-10% might need reclassification
                estimated_reclassifications = int(sample_size * 0.075)  # 7.5%
                validation["reclassifications"] = estimated_reclassifications
                validation["accuracy_score"] = 100.0 - (estimated_reclassifications / sample_size * 100)

                if estimated_reclassifications > 0:
                    validation["issues"].append({
                        "type": "classification_accuracy",
                        "severity": "medium",
                        "description": f"Estimated {estimated_reclassifications} documents may need reclassification",
                        "confidence": "estimate_only"
                    })
            else:
                # TODO: Implement actual re-classification with Claude Haiku
                # This would use the parallel_classifier.py with sample documents
                pass

        logger.info(f"üîç Classification validation: {validation['accuracy_score']:.1f}% estimated accuracy")
        return validation

    def _calculate_compliance_score(self, *analysis_results) -> float:
        """Calculate overall compliance score."""
        scores = []

        for result in analysis_results:
            if "compliance_percentage" in result:
                scores.append(result["compliance_percentage"])
            elif "organization_score" in result:
                scores.append(result["organization_score"])
            elif "accuracy_score" in result:
                scores.append(result["accuracy_score"])
            elif "violations" in result:
                # Path validation - convert violations to score
                # Assume good score if no violations
                scores.append(100.0 if result["violations"] == 0 else 85.0)
            else:
                # Default weight for other analyses
                scores.append(90.0)

        return sum(scores) / len(scores) if scores else 0.0

    def _generate_recommendations(self, *analysis_results) -> List[str]:
        """Generate actionable recommendations."""
        recommendations = []

        for result in analysis_results:
            if result.get("missing_folders"):
                recommendations.append(
                    f"Create {len(result['missing_folders'])} missing QB account folders"
                )

            if result.get("violations", 0) > 0:
                recommendations.append(
                    f"Fix {result['violations']} Windows path length violations"
                )

            if result.get("cleanup_actions"):
                recommendations.append(
                    f"Clean up {len(result['cleanup_actions'])} system/temp files"
                )

            if result.get("misplaced_files", 0) > 0:
                recommendations.append(
                    f"Reorganize {result['misplaced_files']} misplaced documents"
                )

            if result.get("naming_violations", 0) > 0:
                recommendations.append(
                    f"Standardize naming for {result['naming_violations']} files"
                )

        if not recommendations:
            recommendations.append("QB organization is compliant - no major issues found")

        return recommendations

    async def _save_validation_report(self, result: ValidationResult) -> None:
        """Save comprehensive validation report."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = Path(f"qb_validation_report_{timestamp}.json")

        report_data = asdict(result)

        try:
            with open(report_file, 'w') as f:
                json.dump(report_data, f, indent=2, default=str)
            logger.info(f"[SCAN] Validation report saved: {report_file}")
        except Exception as e:
            logger.error(f"‚ùå Failed to save validation report: {str(e)}")

    # Helper methods
    def _is_temp_file(self, filename: str) -> bool:
        """Check if file is a temporary file."""
        return filename.startswith('tmpclaude-') or filename == 'nul'

    def _is_system_file(self, filename: str) -> bool:
        """Check if file is a system file."""
        system_files = ['desktop.ini', 'thumbs.db', '.ds_store']
        return filename.lower() in system_files

    def _is_business_document(self, file_path: Path) -> bool:
        """Check if file is a business document."""
        # Use same logic as parallel classifier
        extension = file_path.suffix.lower()
        business_extensions = {'.pdf', '.png', '.jpg', '.jpeg', '.tiff', '.bmp'}
        return extension in business_extensions

    def _get_size_category(self, size_mb: float) -> str:
        """Categorize file size."""
        if size_mb < 1:
            return "small"
        elif size_mb < 10:
            return "medium"
        else:
            return "large"

    def _get_all_qb_folders(self) -> List[str]:
        """Get all QB folder paths."""
        folders = []
        for category, subfolders in self.qb_structure.items():
            folders.append(category)
            folders.extend(subfolders)
        return folders

    def _follows_qb_naming_convention(self, filename: str) -> bool:
        """Check if filename follows QB naming convention."""
        # QB convention: YYYYMMDD_HHMMSS_Vendor_Amount_Invoice.pdf
        pattern = r'\d{8}_\d{6}_.+_\d+\.\d+_.+\.pdf'
        return bool(re.match(pattern, filename))

    def _is_in_appropriate_qb_folder(self, file_path: Path) -> bool:
        """Check if file is in appropriate QB folder structure."""
        # Simple check - file should be in a QB account subfolder
        path_parts = file_path.parts
        for part in path_parts:
            if any(qb_folder in part for qb_folder in self._get_all_qb_folders()):
                return True
        return False

async def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(
        description="QB Organization Validation & Remediation System"
    )
    parser.add_argument(
        "--target-dir",
        default=settings.CONSOLIDATION_TARGET_DIR,
        help="Target directory for validation"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=True,
        help="Perform dry run validation without changes"
    )
    parser.add_argument(
        "--comprehensive",
        action="store_true",
        help="Run comprehensive validation including classification checks"
    )
    parser.add_argument(
        "--backup",
        action="store_true",
        help="Create backup before any changes"
    )
    parser.add_argument(
        "--remediate",
        action="store_true",
        help="Enable remediation actions (requires --backup)"
    )

    args = parser.parse_args()

    # Validate arguments
    if args.remediate and not args.backup:
        logger.error("‚ùå Remediation requires --backup flag for safety")
        sys.exit(1)

    if args.remediate:
        args.dry_run = False

    # Initialize validation engine
    validator = QBValidationEngine(args.target_dir, args.dry_run)

    try:
        # Run validation
        result = await validator.run_comprehensive_validation()

        # Display results
        print(f"\n{'='*60}")
        print(f"üéØ QB VALIDATION RESULTS")
        print(f"{'='*60}")
        print(f"üìÅ Target Directory: {result.target_directory}")
        print(f"[SCAN] Total Files: {result.total_files:,}")
        print(f"üìÑ Business Documents: {result.business_documents:,}")
        print(f"üóÇÔ∏è  System Files: {result.system_files:,}")
        print(f"‚≠ê Compliance Score: {result.compliance_score:.1f}%")
        print(f"‚ö†Ô∏è  Issues Found: {len(result.issues_found)}")
        print(f"üîß Path Violations: {result.path_violations}")
        print(f"üí∞ Estimated Cost: ${result.cost_estimate_usd:.2f}")
        print(f"‚è±Ô∏è  Processing Time: {result.processing_time_seconds:.1f}s")

        print(f"\nüîß RECOMMENDATIONS:")
        for i, rec in enumerate(result.recommendations, 1):
            print(f"  {i}. {rec}")

        if result.issues_found:
            print(f"\n‚ö†Ô∏è  ISSUES DETAILS:")
            for issue in result.issues_found[:5]:  # Show first 5 issues
                print(f"  ‚Ä¢ {issue['type']}: {issue['description']} ({issue['severity']})")

        print(f"\n{'='*60}")

        # Success/failure determination
        if result.compliance_score >= 90.0:
            print("‚úÖ QB organization is highly compliant!")
            sys.exit(0)
        elif result.compliance_score >= 75.0:
            print("‚ö†Ô∏è  QB organization needs minor improvements")
            sys.exit(0)
        else:
            print("‚ùå QB organization requires significant remediation")
            sys.exit(1)

    except Exception as e:
        logger.error(f"‚ùå Validation failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())